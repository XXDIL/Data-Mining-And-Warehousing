[![Generic badge](https://img.shields.io/badge/DATA-MINING-<BLUE>.svg)](https://shields.io/)
[![Generic badge](https://img.shields.io/badge/MACHINE-LEARNING-<BLUE>.svg)](https://shields.io/)
[![Generic badge](https://img.shields.io/badge/LANGUAGE-PYTHON-<BLUE>.svg)](https://shields.io/)

# Data Mining And Warehousing - IIITA

### This is a course at IIITA, taken by Dr. OP Vyas Sir.

In this course we will learn the basics of Data Mining Algorithms, and even dive into some advanced topics.

## [Tutorial](https://github.com/XXDIL/Data-Mining-And-Warehousing/tree/main/Tutorials)

### [Tutorial-1](https://github.com/XXDIL/Data-Mining-And-Warehousing/blob/main/Tutorials/IIT2018179_DMW_Tut1.pdf)

Written practice on Apriori Algorithm


### [Tutorial-2](https://github.com/XXDIL/Data-Mining-And-Warehousing/blob/main/Tutorials/IIT2018179_DMW_Tut2.pdf)

Written practice on FP-Growth Algorithm


## [Lab](https://github.com/XXDIL/Data-Mining-And-Warehousing/tree/main/Labs)

### Lab-1:


1. Implement Apriori algorithm for association rules. Run the algorithm with two different support
and confidence level defined by you. (Cheese, Mushroom, Retail dataset can be used.)

- Print frequent itemset.
- Print closed frequent itemset.

2. Implement Apriori algorithm for association rules using hash function. Run the algorithm with
two different user-defined support and confidence level to find frequent item sets from L2
and C2.

3. Consider a set of items from the alphabet: {A, B, C, D, and E} and the collection of frequent sets S = {{A},{B},{C},{E},{A,B},{A,C},{A,E},{C,E},{A,C,E}} Find negative and positive collection of frequent sets.


4. Use partitioning to divide a data set in two partitions. Apply Apriori algorithm and compare the
frequent pattern results between integrated and partitioned data sets.


Note: Let Y ⊆I and X⊆Y  If the X is an infrequent itemset, then Y is also an infrequent itemset. On that basis apply the
Apriori algorithm.

------

### Lab-2
